# ðŸš€ TFP Flows Unified Submission Guide

## ðŸ“‹ **Quick Start**

### **1. Basic Submission**
```bash
sbatch submit_tfp_array.sh
```

### **2. Check Progress**  
```bash
squeue -u $(whoami)
```

### **3. Scan for Failures**
```bash
./scan_and_resubmit.sh
```

### **4. Resubmit Failures (if any)**
```bash
./resubmit_failed_YYYYMMDD_HHMMSS.sh  # Auto-generated by scan
```

---

## ðŸŽ¯ **Available Scripts**

### **Main Submission: `submit_tfp_array.sh`**
- âœ… **Intelligent GPU allocation** (8 GPUs per node, all used)
- âœ… **Particle size detection** (>100k objects = extended runtime)
- âœ… **Smart resubmission mode** for failed particles
- âœ… **File locking** to prevent race conditions

### **Failure Management: `scan_and_resubmit.sh`**
- âœ… **Comprehensive failure detection** (model + sample files)
- âœ… **Auto-generates resubmission scripts** with optimal parameters
- âœ… **Particle size analysis** for runtime recommendations
- âœ… **Detailed failure reports**

### **Specialized: `brute_force_gpu_job.sh`**
- âœ… **Brute force testing** across multiple halos and PIDs
- âœ… **Kept for specialized testing scenarios**

---

## âš™ï¸ **Configuration Options**

### **Environment Variables**

```bash
# Particle processing
PARTICLES_PER_TASK=8        # GPUs to use (matches requested GPUs)
TOTAL_PARTICLES=1000        # Total particles to process

# Training parameters  
EPOCHS=50                   # Training epochs
BATCH_SIZE=1024            # Batch size
LEARNING_RATE=1e-3         # Learning rate
N_LAYERS=4                 # Flow layers
HIDDEN_UNITS=64            # Hidden units per layer

# Output configuration
OUTPUT_BASE_DIR=/path       # Custom output directory
H5_FILE_OVERRIDE=/path      # Override H5 file discovery

# Resubmission mode
RESUBMIT_MODE=true         # Enable resubmission mode
FAILED_PIDS_LIST="1,5,23"  # Comma-separated failed PIDs
CHECK_SAMPLES=true         # Check sample files in addition to models
```

### **Usage Examples**

```bash
# Custom configuration
EPOCHS=100 BATCH_SIZE=2048 sbatch submit_tfp_array.sh

# Different H5 file
H5_FILE_OVERRIDE=/path/to/specific.h5 sbatch submit_tfp_array.sh

# Scan specific particle range
TOTAL_PARTICLES=500 ./scan_and_resubmit.sh

# Dry run (no actual submission)
DRY_RUN=true ./scan_and_resubmit.sh
```

---

## ðŸŽ® **Particle Size Intelligence**

### **Automatic Detection**
- **Large particles (>100k objects)**: Extended runtime, optimized parameters
- **Small particles (<100k objects)**: Standard runtime, more epochs

### **Adaptive Parameters**
- **Large particles**: 20% fewer epochs, 50% larger batches (efficiency)
- **Small particles**: 20% more epochs (accuracy)
- **Runtime recommendations**: 12h for large, 4h for small

### **Smart Resubmission**
- Automatically detects particle sizes in failed PIDs
- Generates resubmission with appropriate time allocation
- Separates large and small particle resubmissions

---

## ðŸ”„ **Complete Workflow Examples**

### **Scenario 1: Fresh Start**
```bash
# Submit initial job
sbatch submit_tfp_array.sh

# Monitor progress
squeue -u $(whoami)
watch 'squeue -u $(whoami)'

# After completion, check for failures
./scan_and_resubmit.sh

# If failures found, resubmit
./resubmit_failed_20240824_142500.sh
```

### **Scenario 2: Targeted Resubmission**
```bash
# Submit specific failed PIDs
FAILED_PIDS_LIST="1,5,23,88,200" RESUBMIT_MODE=true sbatch --array=1-5%5 submit_tfp_array.sh

# Or let the scanner handle it
./scan_and_resubmit.sh
```

### **Scenario 3: Custom Particle Range**
```bash
# Process only particles 1-500
TOTAL_PARTICLES=500 sbatch --array=1-63%10 submit_tfp_array.sh
# (63 tasks Ã— 8 particles = 504 particles, covers 1-500)

# Scan only those particles
TOTAL_PARTICLES=500 ./scan_and_resubmit.sh
```

---

## ðŸ“Š **Output Structure**

```
/oak/stanford/orgs/kipac/users/caganze/tfp_flows_output/
â”œâ”€â”€ trained_flows/
â”‚   â””â”€â”€ {data_source}/
â”‚       â””â”€â”€ halo{ID}/
â”‚           â”œâ”€â”€ model_pid1.npz
â”‚           â”œâ”€â”€ model_pid1_results.json
â”‚           â”œâ”€â”€ model_pid2.npz
â”‚           â””â”€â”€ model_pid2_results.json
â”œâ”€â”€ samples/
â”‚   â””â”€â”€ {data_source}/
â”‚       â””â”€â”€ halo{ID}/
â”‚           â”œâ”€â”€ model_pid1_samples.npz
â”‚           â””â”€â”€ model_pid2_samples.npz
â””â”€â”€ failed_particles.log
```

---

## ðŸš¨ **Troubleshooting**

### **Jobs Not Starting**
```bash
# Check partition availability
sinfo -p gpu

# Check your job queue
squeue -u $(whoami)

# Check QOS limits
sacctmgr show qos
```

### **Immediate Job Failures**
```bash
# Check recent error logs
tail -50 logs/tfp_*.err

# Check environment
module list
conda env list

# Check GPU availability
nvidia-smi
```

### **File Permission Issues**
```bash
# Check output directory permissions
ls -la /oak/stanford/orgs/kipac/users/caganze/

# Check H5 file access
ls -la /oak/stanford/orgs/kipac/users/caganze/*/eden_scaled_Halo*
```

### **Resubmission Issues**
```bash
# Force scan mode
SCAN_FAILURES=true sbatch --array=0 submit_tfp_array.sh

# Manual failure detection
./scan_and_resubmit.sh

# Check specific particle
python -c "
import h5py
with h5py.File('/path/to/file.h5', 'r') as f:
    print(list(f.keys()))
"
```

---

## ðŸ“ˆ **Performance Tips**

### **Optimize for Your Workload**
- **Many small particles**: Increase `PARTICLES_PER_TASK`
- **Few large particles**: Decrease concurrency (`%5` â†’ `%3`)
- **Mixed sizes**: Let automatic detection handle it

### **Monitor Resource Usage**
```bash
# Check GPU utilization
nvidia-smi

# Check memory usage
free -h

# Check I/O performance  
iostat -x 1
```

### **Queue Management**
```bash
# Check queue status
squeue -p gpu

# Check your priority
sprio -u $(whoami)

# Check resource usage
seff [JOB_ID]
```

---

## ðŸŽ¯ **Best Practices**

1. **Start small**: Test with `--array=1-5%2` first
2. **Monitor closely**: Watch first few tasks for issues
3. **Regular scanning**: Run `scan_and_resubmit.sh` periodically
4. **Keep logs**: Archive successful runs for reference
5. **Check resources**: Ensure adequate disk space and quotas

---

## ðŸ’¡ **Advanced Usage**

### **Custom Array Configurations**
```bash
# Large job with controlled concurrency
sbatch --array=1-1000%5 submit_tfp_array.sh

# Small test
sbatch --array=1-10%2 submit_tfp_array.sh

# Resume from specific task
sbatch --array=50-100%3 submit_tfp_array.sh
```

### **Resource Optimization**
```bash
# High memory allocation
sbatch --mem=256GB submit_tfp_array.sh

# Extended time for large particles
sbatch --time=24:00:00 submit_tfp_array.sh

# Priority queue
sbatch --qos=high submit_tfp_array.sh
```

---

This guide covers the complete workflow for the new unified TFP flows submission system. For additional help, check the individual script headers or run scripts with `--help` where available.
